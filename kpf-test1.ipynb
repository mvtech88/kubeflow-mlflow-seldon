{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47baf33f-32cd-48b3-80fd-727b9e724f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "47444559-fc59-4829-aebb-b61e1320d255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building component using KFP package path: kfp==2.7.0\n",
      "Found 1 component(s) in file /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/data_download/data_download_component.py:\n",
      "Dataset download: ComponentInfo(name='Dataset download', function_name='dataset_download', func=<function dataset_download at 0x108cfe830>, target_image='mohitverma1688/data-download_component:v0.4', module_path=PosixPath('/Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/data_download/data_download_component.py'), component_spec=ComponentSpec(name='dataset-download', implementation=Implementation(container=ContainerSpecImplementation(image='mohitverma1688/data-download_component:v0.4', command=['sh', '-c', '\\nif ! [ -x \"$(command -v pip)\" ]; then\\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \\'pathlib\\' \\'requests\\' \\'kfp-kubernetes\\' && \"$0\" \"$@\"\\n', 'python3', '-m', 'kfp.dsl.executor_main'], args=['--executor_input', '{{$}}', '--function_to_execute', 'dataset_download'], env=None, resources=None), importer=None, graph=None), description=None, inputs={'url': InputSpec(type='String', default=None, optional=False, is_artifact_list=False, description=None), 'base_path': InputSpec(type='String', default=None, optional=False, is_artifact_list=False, description=None)}, outputs=None, platform_spec=), output_component_file=None, base_image='python:3.10-slim', packages_to_install=['pathlib', 'requests', 'kfp-kubernetes'], pip_index_urls=None)\n",
      "Using base image: python:3.10-slim\n",
      "Using target image: mohitverma1688/data-download_component:v0.4\n",
      "Found existing file runtime-requirements.txt under /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/data_download.\n",
      "Overwriting existing file runtime-requirements.txt\n",
      "Generated file /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/data_download/runtime-requirements.txt.\n",
      "Found existing file .dockerignore under /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/data_download.\n",
      "Leaving this file untouched.\n",
      "Found existing file Dockerfile under /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/data_download.\n",
      "Leaving this file untouched.\n",
      "Building image mohitverma1688/data-download_component:v0.4 using Docker...\n",
      "Docker: Step 1/6 : FROM python:3.10-slim\n",
      "Docker:  ---> 61b0911887c0\n",
      "Docker: Step 2/6 : WORKDIR /usr/local/src/kfp/components\n",
      "Docker:  ---> Using cache\n",
      "Docker:  ---> 28190e56082b\n",
      "Docker: Step 3/6 : COPY runtime-requirements.txt runtime-requirements.txt\n",
      "Docker:  ---> b5b1f7bece68\n",
      "Docker: Step 4/6 : RUN pip install --no-cache-dir -r runtime-requirements.txt\n",
      "Docker:  ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n",
      "Docker:  ---> Running in 15a24b84df20\n",
      "Docker: Collecting kfp-kubernetes\n",
      "Docker:   Downloading kfp-kubernetes-1.2.0.tar.gz (15 kB)\n",
      "Docker:   Preparing metadata (setup.py): started\n",
      "Docker:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Docker: Collecting pathlib\n",
      "Docker:   Downloading pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Docker: Collecting requests\n",
      "Docker:   Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.9/64.9 kB 20.8 MB/s eta 0:00:00\n",
      "Docker: Collecting protobuf<5,>=4.21.1\n",
      "Docker:   Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.6/294.6 kB 12.0 MB/s eta 0:00:00\n",
      "Docker: Collecting kfp<3,>=2.6.0\n",
      "Docker:   Downloading kfp-2.8.0.tar.gz (594 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 594.4/594.4 kB 8.9 MB/s eta 0:00:00\n",
      "Docker:   Preparing metadata (setup.py): started\n",
      "Docker:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Docker: Collecting urllib3<3,>=1.21.1\n",
      "Docker:   Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.4/121.4 kB 13.3 MB/s eta 0:00:00\n",
      "Docker: Collecting certifi>=2017.4.17\n",
      "Docker:   Downloading certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 164.4/164.4 kB 7.5 MB/s eta 0:00:00\n",
      "Docker: Collecting idna<4,>=2.5\n",
      "Docker:   Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.8/66.8 kB 17.8 MB/s eta 0:00:00\n",
      "Docker: Collecting charset-normalizer<4,>=2\n",
      "Docker:   Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142.1/142.1 kB 13.7 MB/s eta 0:00:00\n",
      "Docker: Collecting click<9,>=8.0.0\n",
      "Docker:   Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 7.5 MB/s eta 0:00:00\n",
      "Docker: Collecting docstring-parser<1,>=0.7.3\n",
      "Docker:   Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Docker: Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "Docker:   Downloading google_api_core-2.19.1-py3-none-any.whl (139 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.4/139.4 kB 6.9 MB/s eta 0:00:00\n",
      "Docker: Collecting google-auth<3,>=1.6.1\n",
      "Docker:   Downloading google_auth-2.30.0-py2.py3-none-any.whl (193 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.7/193.7 kB 6.7 MB/s eta 0:00:00\n",
      "Docker: Collecting google-cloud-storage<3,>=2.2.1\n",
      "Docker:   Downloading google_cloud_storage-2.17.0-py2.py3-none-any.whl (126 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 5.3 MB/s eta 0:00:00\n",
      "Docker: Collecting kfp-pipeline-spec==0.3.0\n",
      "Docker:   Downloading kfp_pipeline_spec-0.3.0-py3-none-any.whl (12 kB)\n",
      "Docker: Collecting kfp-server-api<2.1.0,>=2.0.0\n",
      "Docker:   Downloading kfp-server-api-2.0.5.tar.gz (63 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.4/63.4 kB 5.3 MB/s eta 0:00:00\n",
      "Docker:   Preparing metadata (setup.py): started\n",
      "Docker:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Docker: Collecting kubernetes<27,>=8.0.0\n",
      "Docker:   Downloading kubernetes-26.1.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 5.4 MB/s eta 0:00:00\n",
      "Docker: Collecting PyYAML<7,>=5.3\n",
      "Docker:   Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 705.5/705.5 kB 5.1 MB/s eta 0:00:00\n",
      "Docker: Collecting requests-toolbelt<1,>=0.8.0\n",
      "Docker:   Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 kB 13.0 MB/s eta 0:00:00\n",
      "Docker: Collecting tabulate<1,>=0.8.6\n",
      "Docker:   Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Docker: Collecting urllib3<3,>=1.21.1\n",
      "Docker:   Downloading urllib3-1.26.19-py2.py3-none-any.whl (143 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.9/143.9 kB 5.2 MB/s eta 0:00:00\n",
      "Docker: Collecting proto-plus<2.0.0dev,>=1.22.3\n",
      "Docker:   Downloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.1/50.1 kB 26.5 MB/s eta 0:00:00\n",
      "Docker: Collecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "Docker:   Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 220.0/220.0 kB 4.4 MB/s eta 0:00:00\n",
      "Docker: Collecting pyasn1-modules>=0.2.1\n",
      "Docker:   Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.2/181.2 kB 4.8 MB/s eta 0:00:00\n",
      "Docker: Collecting rsa<5,>=3.1.4\n",
      "Docker:   Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Docker: Collecting cachetools<6.0,>=2.0.0\n",
      "Docker:   Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Docker: Collecting google-crc32c<2.0dev,>=1.0\n",
      "Docker:   Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Docker: Collecting google-resumable-media>=2.6.0\n",
      "Docker:   Downloading google_resumable_media-2.7.1-py2.py3-none-any.whl (81 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.2/81.2 kB 19.6 MB/s eta 0:00:00\n",
      "Docker: Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "Docker:   Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Docker: Collecting six>=1.10\n",
      "Docker:   Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Docker: Collecting python-dateutil\n",
      "Docker:   Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 6.2 MB/s eta 0:00:00\n",
      "Docker: Collecting requests-oauthlib\n",
      "Docker:   Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Docker: Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp<3,>=2.6.0->kfp-kubernetes->-r runtime-requirements.txt (line 2)) (65.5.1)\n",
      "Docker: Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "Docker:   Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 25.7 MB/s eta 0:00:00\n",
      "Docker: Collecting pyasn1<0.7.0,>=0.4.6\n",
      "Docker:   Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.3/85.3 kB 9.1 MB/s eta 0:00:00\n",
      "Docker: Collecting oauthlib>=3.0.0\n",
      "Docker:   Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 6.6 MB/s eta 0:00:00\n",
      "Docker: Building wheels for collected packages: kfp-kubernetes, kfp, kfp-server-api\n",
      "Docker:   Building wheel for kfp-kubernetes (setup.py): started\n",
      "Docker:   Building wheel for kfp-kubernetes (setup.py): finished with status 'done'\n",
      "Docker:   Created wheel for kfp-kubernetes: filename=kfp_kubernetes-1.2.0-py3-none-any.whl size=20200 sha256=5dcbeced4818c18c85f78e819836ce01e5c3f3513d1a57d8c3e4d90aad5f74a8\n",
      "Docker:   Stored in directory: /tmp/pip-ephem-wheel-cache-2_uozcqd/wheels/02/89/d8/a70d6ad78647244404221d792f576fc95d3ca9b62567406abf\n",
      "Docker:   Building wheel for kfp (setup.py): started\n",
      "Docker:   Building wheel for kfp (setup.py): finished with status 'done'\n",
      "Docker:   Created wheel for kfp: filename=kfp-2.8.0-py3-none-any.whl size=660398 sha256=d1b1815f8ab6a25313f89401105422fb908d0843b4c34919522942568e515977\n",
      "Docker:   Stored in directory: /tmp/pip-ephem-wheel-cache-2_uozcqd/wheels/e1/ba/81/eaaf9e80e8238464c06ef8ecca07150090024887d82944a3ac\n",
      "Docker:   Building wheel for kfp-server-api (setup.py): started\n",
      "Docker:   Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "Docker:   Created wheel for kfp-server-api: filename=kfp_server_api-2.0.5-py3-none-any.whl size=114733 sha256=42c0e07a785790c0dea1acc66e3a3a050b898260c6f8e7106d850fa12d5fd5c9\n",
      "Docker:   Stored in directory: /tmp/pip-ephem-wheel-cache-2_uozcqd/wheels/ac/4f/f0/2f622aadcbf8921fb72d24f52efaffacc235f863c195c289c5\n",
      "Docker: Successfully built kfp-kubernetes kfp kfp-server-api\n",
      "Docker: Installing collected packages: pathlib, websocket-client, urllib3, tabulate, six, PyYAML, pyasn1, protobuf, oauthlib, idna, google-crc32c, docstring-parser, click, charset-normalizer, certifi, cachetools, rsa, requests, python-dateutil, pyasn1-modules, proto-plus, kfp-pipeline-spec, googleapis-common-protos, google-resumable-media, requests-toolbelt, requests-oauthlib, kfp-server-api, google-auth, kubernetes, google-api-core, google-cloud-core, google-cloud-storage, kfp, kfp-kubernetes\n",
      "Docker: Successfully installed PyYAML-6.0.1 cachetools-5.3.3 certifi-2024.6.2 charset-normalizer-3.3.2 click-8.1.7 docstring-parser-0.16 google-api-core-2.19.1 google-auth-2.30.0 google-cloud-core-2.4.1 google-cloud-storage-2.17.0 google-crc32c-1.5.0 google-resumable-media-2.7.1 googleapis-common-protos-1.63.2 idna-3.7 kfp-2.8.0 kfp-kubernetes-1.2.0 kfp-pipeline-spec-0.3.0 kfp-server-api-2.0.5 kubernetes-26.1.0 oauthlib-3.2.2 pathlib-1.0.1 proto-plus-1.24.0 protobuf-4.25.3 pyasn1-0.6.0 pyasn1-modules-0.4.0 python-dateutil-2.9.0.post0 requests-2.32.3 requests-oauthlib-2.0.0 requests-toolbelt-0.10.1 rsa-4.9 six-1.16.0 tabulate-0.9.0 urllib3-1.26.19 websocket-client-1.8.0\n",
      "Docker: \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "Docker: \u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.1.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0m\n",
      "Docker:  ---> cb3e0e6300b8\n",
      "Docker: Step 5/6 : RUN pip install --no-cache-dir kfp==2.7.0\n",
      "Docker:  ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n",
      "Docker:  ---> Running in a591440319c3\n",
      "Docker: Collecting kfp==2.7.0\n",
      "Docker:   Downloading kfp-2.7.0.tar.gz (441 kB)\n",
      "Docker:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 441.8/441.8 kB 671.1 kB/s eta 0:00:00\n",
      "Docker:   Preparing metadata (setup.py): started\n",
      "Docker:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Docker: Requirement already satisfied: click<9,>=8.0.0 in /usr/local/lib/python3.10/site-packages (from kfp==2.7.0) (8.1.7)\n",
      "Docker: Requirement already satisfied: docstring-parser<1,>=0.7.3 in /usr/local/lib/python3.10/site-packages (from kfp==2.7.0) (0.16)\n",
      "Docker: Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/site-packages (from kfp==2.7.0) (2.19.1)\n",
      "Docker: Requirement already satisfied: google-auth<3,>=1.6.1 in /usr/local/lib/python3.10/site-packages (from kfp==2.7.0) (2.30.0)\n",
      "Docker: Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /usr/local/lib/python3.10/site-packages (from kfp==2.7.0) (2.17.0)\n",
      "Docker: Requirement already satisfied: kfp-pipeline-spec==0.3.0 in /usr/local/lib/python3.10/site-packages (from kfp==2.7.0) (0.3.0)\n",
      "Docker: Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from kfp==2.7.0) (2.0.5)\n",
      "Docker: Requirement already satisfied: kubernetes<27,>=8.0.0 in /usr/local/lib/python3.10/site-packages (from kfp==2.7.0) (26.1.0)\n",
      "Docker: Requirement already satisfied: protobuf<5,>=4.21.1 in /usr/local/lib/python3.10/site-packages (from kfp==2.7.0) (4.25.3)\n",
      "Docker: Requirement already satisfied: PyYAML<7,>=5.3 in /usr/local/lib/python3.10/site-packages (from kfp==2.7.0) (6.0.1)\n",
      "Docker: Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /usr/local/lib/python3.10/site-packages (from kfp==2.7.0) (0.10.1)\n",
      "Docker: Requirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.10/site-packages (from kfp==2.7.0) (0.9.0)\n",
      "Docker: Requirement already satisfied: urllib3<2.0.0 in /usr/local/lib/python3.10/site-packages (from kfp==2.7.0) (1.26.19)\n",
      "Docker: Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.7.0) (1.63.2)\n",
      "Docker: Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.7.0) (2.32.3)\n",
      "Docker: Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.7.0) (1.24.0)\n",
      "Docker: Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp==2.7.0) (4.9)\n",
      "Docker: Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp==2.7.0) (5.3.3)\n",
      "Docker: Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp==2.7.0) (0.4.0)\n",
      "Docker: Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.7.0) (1.5.0)\n",
      "Docker: Requirement already satisfied: google-resumable-media>=2.6.0 in /usr/local/lib/python3.10/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.7.0) (2.7.1)\n",
      "Docker: Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.7.0) (2.4.1)\n",
      "Docker: Requirement already satisfied: certifi in /usr/local/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.7.0) (2024.6.2)\n",
      "Docker: Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.7.0) (1.16.0)\n",
      "Docker: Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.7.0) (2.9.0.post0)\n",
      "Docker: Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp==2.7.0) (65.5.1)\n",
      "Docker: Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp==2.7.0) (2.0.0)\n",
      "Docker: Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp==2.7.0) (1.8.0)\n",
      "Docker: Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp==2.7.0) (0.6.0)\n",
      "Docker: Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.7.0) (3.7)\n",
      "Docker: Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.7.0) (3.3.2)\n",
      "Docker: Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp==2.7.0) (3.2.2)\n",
      "Docker: Building wheels for collected packages: kfp\n",
      "Docker:   Building wheel for kfp (setup.py): started\n",
      "Docker:   Building wheel for kfp (setup.py): finished with status 'done'\n",
      "Docker:   Created wheel for kfp: filename=kfp-2.7.0-py3-none-any.whl size=610419 sha256=6e176885ec3c2a3d9a5a2e85fed221bd345055c70324aaa6e0918048f7cdb0b9\n",
      "Docker:   Stored in directory: /tmp/pip-ephem-wheel-cache-ppababm4/wheels/9e/7d/a4/f9d013e82681c9746ef10de3b00456163577a99279c5ed673d\n",
      "Docker: Successfully built kfp\n",
      "Docker: Installing collected packages: kfp\n",
      "Docker:   Attempting uninstall: kfp\n",
      "Docker:     Found existing installation: kfp 2.8.0\n",
      "Docker:     Uninstalling kfp-2.8.0:\n",
      "Docker:       Successfully uninstalled kfp-2.8.0\n",
      "Docker: Successfully installed kfp-2.7.0\n",
      "Docker: \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "Docker: \u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.1.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0m\n",
      "Docker:  ---> ef1116e3c22c\n",
      "Docker: Step 6/6 : COPY . .\n",
      "Docker:  ---> 1b85ba5de09f\n",
      "Docker: Successfully built 1b85ba5de09f\n",
      "Docker: Successfully tagged mohitverma1688/data-download_component:v0.4\n",
      "Pushing image mohitverma1688/data-download_component:v0.4...\n",
      "Docker:  The push refers to repository [docker.io/mohitverma1688/data-download_component]\n",
      "Docker: fbbfc931fe9d Preparing\n",
      "Docker: adc9c9cc0b77 Preparing\n",
      "Docker: 9b9e594b279c Preparing\n",
      "Docker: ba2c5335f1c7 Preparing\n",
      "Docker: 7565472a3ca8 Preparing\n",
      "Docker: 6020d42d2f73 Preparing\n",
      "Docker: 085963967517 Preparing\n",
      "Docker: a2d5e7388727 Preparing\n",
      "Docker: 146826fa3ca0 Preparing\n",
      "Docker: 5d4427064ecc Preparing\n",
      "Docker: 6020d42d2f73 Waiting\n",
      "Docker: 085963967517 Waiting\n",
      "Docker: a2d5e7388727 Waiting\n",
      "Docker: 146826fa3ca0 Waiting\n",
      "Docker: 5d4427064ecc Waiting\n",
      "Docker: fbbfc931fe9d Pushing\n",
      "Docker: fbbfc931fe9d Pushing\n",
      "Docker: ba2c5335f1c7 Pushing\n",
      "Docker: ba2c5335f1c7 Pushing\n",
      "Docker: adc9c9cc0b77 Pushing\n",
      "Docker: 7565472a3ca8 Layer already exists\n",
      "Docker: 9b9e594b279c Pushing\n",
      "Docker: adc9c9cc0b77 Pushing\n",
      "Docker: 9b9e594b279c Pushing\n",
      "Docker: adc9c9cc0b77 Pushing\n",
      "Docker: adc9c9cc0b77 Pushing\n",
      "Docker: 9b9e594b279c Pushing\n",
      "Docker: 9b9e594b279c Pushing\n",
      "Docker: 9b9e594b279c Pushing\n",
      "Docker: 9b9e594b279c Pushing\n",
      "Docker: 9b9e594b279c Pushing\n",
      "Docker: 9b9e594b279c Pushing\n",
      "Docker: 9b9e594b279c Pushing\n",
      "Docker: 9b9e594b279c Pushing\n",
      "Docker: 9b9e594b279c Pushing\n",
      "Docker: 9b9e594b279c Pushing\n",
      "Docker: 9b9e594b279c Pushing\n",
      "Docker: 9b9e594b279c Pushing\n",
      "Docker: 9b9e594b279c Pushing\n",
      "Docker: 6020d42d2f73 Layer already exists\n",
      "Docker: 085963967517 Layer already exists\n",
      "Docker: fbbfc931fe9d Pushed\n",
      "Docker: ba2c5335f1c7 Pushed\n",
      "Docker: a2d5e7388727 Layer already exists\n",
      "Docker: 5d4427064ecc Layer already exists\n",
      "Docker: 146826fa3ca0 Layer already exists\n",
      "Docker: adc9c9cc0b77 Pushed\n",
      "Docker: 9b9e594b279c Pushed\n",
      "Docker:  v0.4: digest: sha256:08ea39a985b0c44f06a3f068319b0831c0aec1dd53588e42d31526dc5bb167e0 size: 2415\n",
      "Built and pushed component container mohitverma1688/data-download_component:v0.4\n"
     ]
    }
   ],
   "source": [
    "!kfp component build src/components/data_download --component-filepattern data_download_component.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6724c73d-5679-4fcd-97b5-f4d369e5eff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/components/data_download/data_download_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/components/data_download/data_download_component.py\n",
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image='python:3.10-slim',\n",
    "               target_image='mohitverma1688/data-download_component:v0.4',\n",
    "               packages_to_install=['pathlib','requests','kfp-kubernetes'])\n",
    "\n",
    "\n",
    "\n",
    "def dataset_download(url: str, base_path:str, \n",
    "                     ):\n",
    "\n",
    "    import os\n",
    "    import requests\n",
    "    import zipfile\n",
    "    from pathlib import Path\n",
    "\n",
    "    #Import the enviornment variable \n",
    "\n",
    "    #os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio-service.kubeflow:9000\"\n",
    "    #os.environ[\"AWS_ACCESS_KEY_ID\"] = aws_access_key_id\n",
    "    #os.environ[\"AWS_SECRET_ACCESS_KEY\"] = aws_secret_access_key\n",
    "    \n",
    "    #s3 = boto3.client(\n",
    "    #    \"s3\",\n",
    "    #    endpoint_url=\"http://minio-service.kubeflow:9000\",\n",
    "    #    aws_access_key_id=aws_access_key_id,\n",
    "    #    aws_secret_access_key=aws_secret_access_key,\n",
    "    #    config=Config(signature_version=\"s3v4\"),\n",
    "    #)\n",
    "    # Create data bucket if it does not yet exist\n",
    "    #response = s3.list_buckets()\n",
    "    #input_bucket_exists = False\n",
    "    #for bucket in response[\"Buckets\"]:\n",
    "    #    if bucket[\"Name\"] == input_bucket:\n",
    "    #        input_bucket_exists = True\n",
    "    #        \n",
    "    #if not input_bucket_exists:\n",
    "    #    s3.create_bucket(ACL=\"public-read-write\", Bucket=input_bucket)\n",
    "\n",
    "\n",
    "    # Save zip files to S3 import_bucket\n",
    "    data_path = Path(base_path)\n",
    "    \n",
    "    if data_path.is_dir():\n",
    "      print(f\"{data_path} directory exists.\")\n",
    "    else:\n",
    "      print(f\"Did not find {data_path} directory, creating one...\")\n",
    "      data_path.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "\n",
    "    # Download pizza , steak and sushi data and upload to the s3 bucket. This is example code to save the downloaded data to the bucket\n",
    "    with open(f\"{data_path}/data.zip\", \"wb\") as f:\n",
    "        request = requests.get(f\"{url}\")\n",
    "        print(f\"Downloading data from {url}...\")\n",
    "        f.write(request.content)\n",
    "    #    for root, dir, files in os.walk(data_path):\n",
    "    #        for filename in files:\n",
    "    #            local_path = os.path.join(root,filename)\n",
    "    #            s3.upload_file(\n",
    "    #               local_path,\n",
    "    #               input_bucket,\n",
    "    #               f\"{local_path}\",\n",
    "    #               ExtraArgs={\"ACL\": \"public-read\"},\n",
    "    #             )\n",
    "\n",
    "    # unzip the data to use the data in the next step. Data will be stored in PVC.\n",
    "    with zipfile.ZipFile(data_path/\"data.zip\", \"r\") as zip_ref:\n",
    "      print(\"Unzipping data...\")\n",
    "      zip_ref.extractall(data_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed40883b-a766-446b-9880-710353d73ed5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building component using KFP package path: kfp==2.7.0\n",
      "Found 1 component(s) in file /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_train_cnn/model_train_component.py:\n",
      "Model train: ComponentInfo(name='Model train', function_name='model_train', func=<function model_train at 0x106f9e830>, target_image='mohitverma1688/model_train_component:v0.24', module_path=PosixPath('/Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_train_cnn/model_train_component.py'), component_spec=ComponentSpec(name='model-train', implementation=Implementation(container=ContainerSpecImplementation(image='mohitverma1688/model_train_component:v0.24', command=['sh', '-c', '\\nif ! [ -x \"$(command -v pip)\" ]; then\\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \\'pandas\\' \\'matplotlib\\' && \"$0\" \"$@\"\\n', 'python3', '-m', 'kfp.dsl.executor_main'], args=['--executor_input', '{{$}}', '--function_to_execute', 'model_train'], env=None, resources=None), importer=None, graph=None), description=None, inputs={'num_epochs': InputSpec(type='Integer', default=None, optional=False, is_artifact_list=False, description=None), 'batch_size': InputSpec(type='Integer', default=None, optional=False, is_artifact_list=False, description=None), 'hidden_units': InputSpec(type='Integer', default=None, optional=False, is_artifact_list=False, description=None), 'learning_rate': InputSpec(type='Float', default=None, optional=False, is_artifact_list=False, description=None), 'train_dir': InputSpec(type='String', default=None, optional=False, is_artifact_list=False, description=None), 'test_dir': InputSpec(type='String', default=None, optional=False, is_artifact_list=False, description=None), 'model_name': InputSpec(type='String', default=None, optional=False, is_artifact_list=False, description=None), 'model_dir': InputSpec(type='String', default=None, optional=False, is_artifact_list=False, description=None), 'export_bucket': InputSpec(type='String', default='modelbucket', optional=True, is_artifact_list=False, description=None)}, outputs={'model_artifact_path': OutputSpec(type='system.Model@0.0.1', is_artifact_list=False, description=None), 'parameters_json_path': OutputSpec(type='system.Artifact@0.0.1', is_artifact_list=False, description=None), 'train_loss': OutputSpec(type='system.HTML@0.0.1', is_artifact_list=False, description=None), 'Output': OutputSpec(type='typing.Dict[str, list]', is_artifact_list=False, description=None)}, platform_spec=), output_component_file=None, base_image='mohitverma1688/model_train_component:v0.4', packages_to_install=['pandas', 'matplotlib'], pip_index_urls=None)\n",
      "Using base image: mohitverma1688/model_train_component:v0.4\n",
      "Using target image: mohitverma1688/model_train_component:v0.24\n",
      "Found existing file runtime-requirements.txt under /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_train_cnn.\n",
      "Overwriting existing file runtime-requirements.txt\n",
      "Generated file /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_train_cnn/runtime-requirements.txt.\n",
      "Found existing file .dockerignore under /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_train_cnn.\n",
      "Leaving this file untouched.\n",
      "Found existing file Dockerfile under /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_train_cnn.\n",
      "Leaving this file untouched.\n",
      "Building image mohitverma1688/model_train_component:v0.24 using Docker...\n",
      "Docker: Step 1/6 : FROM mohitverma1688/model_train_component:v0.2\n",
      "Docker:  ---> 01ee800e1f91\n",
      "Docker: Step 2/6 : WORKDIR /usr/local/src/kfp/components\n",
      "Docker:  ---> Running in 220f33cb9f73\n",
      "Docker:  ---> c75b6e9a9a4b\n",
      "Docker: Step 3/6 : COPY runtime-requirements.txt runtime-requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!kfp component build src/components/model_train_cnn --component-filepattern model_train_component.py --no-push-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24b3c23f-500a-4b25-85ad-d035d3deb8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/components/model_train_cnn/model_train_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/components/model_train_cnn/model_train_component.py\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "from typing import Dict\n",
    "from kfp.dsl import Dataset,Output,Artifact,OutputPath,InputPath, Model,HTML\n",
    "\n",
    "@dsl.component(base_image='mohitverma1688/model_train_component:v0.4',\n",
    "               target_image='mohitverma1688/model_train_component:v0.24',\n",
    "               packages_to_install=['pandas','matplotlib']\n",
    "               )\n",
    "\n",
    "def model_train(num_epochs:int, \n",
    "                batch_size:int, \n",
    "                hidden_units:int,\n",
    "                learning_rate: float,\n",
    "                train_dir: str,\n",
    "                test_dir: str,\n",
    "                model_name: str,\n",
    "                model_dir: str,\n",
    "                model_artifact_path: OutputPath('Model'),\n",
    "                parameters_json_path: OutputPath('Artifact'),\n",
    "                train_loss: Output[HTML],\n",
    "                export_bucket: str = \"modelbucket\", \n",
    "               ) -> Dict[str, list] :\n",
    "\n",
    "            import os\n",
    "            import json\n",
    "            import pandas as pd\n",
    "            import torch\n",
    "            import data_setup, model_train, model_builder, utils\n",
    "            from io import BytesIO\n",
    "            import base64\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            from torchvision import transforms\n",
    "         \n",
    "            \n",
    "\n",
    "            # Setup hyperparameters\n",
    "            NUM_EPOCHS = num_epochs\n",
    "            BATCH_SIZE = batch_size\n",
    "            HIDDEN_UNITS = hidden_units\n",
    "            LEARNING_RATE = learning_rate\n",
    "            MODEL_NAME = model_name\n",
    "            MODEL_DIR = model_dir\n",
    "            EXPORT_BUCKET = export_bucket\n",
    "\n",
    "    \n",
    "\n",
    "            # Setup directories\n",
    "            TRAIN_DIR = train_dir\n",
    "            TEST_DIR = test_dir\n",
    "\n",
    "            # Setup target device\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "            # Create transforms\n",
    "            data_transform = transforms.Compose([\n",
    "              transforms.Resize((64, 64)),\n",
    "              transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "            # Create DataLoaders with help from data_setup.py\n",
    "            train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "                train_dir=TRAIN_DIR,\n",
    "                test_dir=TEST_DIR,\n",
    "                transform=data_transform,\n",
    "                batch_size=BATCH_SIZE\n",
    "            )\n",
    "\n",
    "            # Create model with help from model_builder.py\n",
    "            model = model_builder.TinyVGG(\n",
    "                input_shape=3,\n",
    "                hidden_units=HIDDEN_UNITS,\n",
    "                output_shape=len(class_names)\n",
    "            ).to(device)\n",
    "\n",
    "            # Set loss and optimizer\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                         lr=LEARNING_RATE)\n",
    "\n",
    "            # Start training with help from engine.py\n",
    "            result = model_train.train(model=model,\n",
    "                         train_dataloader=train_dataloader,\n",
    "                         loss_fn=loss_fn,\n",
    "                         optimizer=optimizer,\n",
    "                         epochs=NUM_EPOCHS,\n",
    "                         device=device)\n",
    "    \n",
    "             \n",
    "\n",
    "            df = pd.DataFrame(result)\n",
    "    \n",
    "            # Example Saving dataset as an artifact in kubeflow pipeline. \n",
    "            #output_dataset.path = f\"{output_dataset.path}.csv\"\n",
    "            #df.to_csv(output_dataset.path, index=False)\n",
    "            #with open(output_dataset.path, \"w\") as file:\n",
    "            #file.write(df.to_csv(index=False))\n",
    "        \n",
    "\n",
    "            #Get number of epochs\n",
    "\n",
    "            epochs = range(len(df))\n",
    "        \n",
    "\n",
    "            # Plot train loss in the kfp pipeline for visialization. \n",
    "            tmpfile = BytesIO()\n",
    "            plt.figure(figsize=(15,10))\n",
    "            plt.subplot(2,2,1)\n",
    "            plt.plot(epochs, df[\"train_loss\"], label={model_name})\n",
    "            plt.title(\"Train Loss\")\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.legend()\n",
    "            plt.savefig(tmpfile, format=\"png\")\n",
    "            tmpfile.seek(0)\n",
    "            encoded = base64.b64encode(tmpfile.read()).decode(\"utf-8\")\n",
    "            train_loss.path = f\"{train_loss.path}.html\"\n",
    "            html = f\"<img src='data:image/png;base64,{encoded}'>\"\n",
    "            with open(train_loss.path, 'w') as f:\n",
    "                f.write(html)\n",
    "\n",
    "                \n",
    "                \n",
    "            # Save the model with help from utils.py on the local PVC and if export_bucket is provided then on minio storage also.\n",
    "            utils.save_model(model=model,\n",
    "                             model_dir=MODEL_DIR,\n",
    "                             model_name=MODEL_NAME + \".pth\",\n",
    "                             export_bucket=EXPORT_BUCKET)\n",
    "\n",
    "            print(\"saving to kfp now\")\n",
    "        \n",
    "            # Saving the model as kfp output artifcat. \n",
    "    \n",
    "            torch.save(model.state_dict(),\n",
    "                       model_artifact_path)\n",
    "\n",
    "            # Below steps logs the parameters to be used in the next step to be used by mlflow server\n",
    "            with open(parameters_json_path, 'w') as f:\n",
    "                json.dump({'lr': learning_rate, 'batch_size': batch_size, 'epochs': num_epochs}, f)\n",
    "        \n",
    "            # return the dictionary , \n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d6fabd97-abbe-4568-9e20-8ca87ff25db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building component using KFP package path: kfp==2.7.0\n",
      "Found 1 component(s) in file /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_inference/model_inference_component.py:\n",
      "Model inference: ComponentInfo(name='Model inference', function_name='model_inference', func=<function model_inference at 0x11035e830>, target_image='mohitverma1688/model_inference_component:v0.20', module_path=PosixPath('/Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_inference/model_inference_component.py'), component_spec=ComponentSpec(name='model-inference', implementation=Implementation(container=ContainerSpecImplementation(image='mohitverma1688/model_inference_component:v0.20', command=['sh', '-c', '\\nif ! [ -x \"$(command -v pip)\" ]; then\\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \\'pandas\\' \\'matplotlib\\' && \"$0\" \"$@\"\\n', 'python3', '-m', 'kfp.dsl.executor_main'], args=['--executor_input', '{{$}}', '--function_to_execute', 'model_inference'], env=None, resources=None), importer=None, graph=None), description=None, inputs={'model_artifact_path': InputSpec(type='system.Model@0.0.1', default=None, optional=False, is_artifact_list=False, description=None), 'num_epochs': InputSpec(type='Integer', default=None, optional=False, is_artifact_list=False, description=None), 'batch_size': InputSpec(type='Integer', default=None, optional=False, is_artifact_list=False, description=None), 'learning_rate': InputSpec(type='Float', default=None, optional=False, is_artifact_list=False, description=None), 'train_dir': InputSpec(type='String', default=None, optional=False, is_artifact_list=False, description=None), 'test_dir': InputSpec(type='String', default=None, optional=False, is_artifact_list=False, description=None), 'model_name': InputSpec(type='String', default=None, optional=False, is_artifact_list=False, description=None)}, outputs={'test_loss': OutputSpec(type='system.HTML@0.0.1', is_artifact_list=False, description=None), 'metrics_json_path': OutputSpec(type='system.Artifact@0.0.1', is_artifact_list=False, description=None), 'Output': OutputSpec(type='typing.Dict[str, str]', is_artifact_list=False, description=None)}, platform_spec=), output_component_file=None, base_image='mohitverma1688/model_train_component:v0.1', packages_to_install=['pandas', 'matplotlib'], pip_index_urls=None)\n",
      "Using base image: mohitverma1688/model_train_component:v0.1\n",
      "Using target image: mohitverma1688/model_inference_component:v0.20\n",
      "Found existing file runtime-requirements.txt under /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_inference.\n",
      "Overwriting existing file runtime-requirements.txt\n",
      "Generated file /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_inference/runtime-requirements.txt.\n",
      "Found existing file .dockerignore under /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_inference.\n",
      "Leaving this file untouched.\n",
      "Found existing file Dockerfile under /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_inference.\n",
      "Leaving this file untouched.\n",
      "Building image mohitverma1688/model_inference_component:v0.20 using Docker...\n",
      "Docker: Step 1/6 : FROM mohitverma1688/model_train_component:v0.1\n",
      "Docker:  ---> 81bbc346d378\n",
      "Docker: Step 2/6 : WORKDIR /usr/local/src/kfp/components\n",
      "Docker:  ---> Running in 5a146db70a19\n",
      "Docker:  ---> 3659c47fedb6\n",
      "Docker: Step 3/6 : COPY runtime-requirements.txt runtime-requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!kfp component build src/components/model_inference --component-filepattern model_inference_component.py --no-push-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "538543b4-1d7d-42be-a206-47155008dedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/components/model_inference/model_inference_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/components/model_inference/model_inference_component.py\n",
    "\n",
    "# Creating a python script to predict a model\n",
    "\n",
    "from typing import Dict, NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Dataset,Output,Artifact,OutputPath,InputPath, Model,Input,HTML\n",
    "\n",
    "@dsl.component(base_image='mohitverma1688/model_train_component:v0.1',\n",
    "               target_image='mohitverma1688/model_inference_component:v0.20',\n",
    "               packages_to_install=['pandas','matplotlib']\n",
    "               )\n",
    "\n",
    "def model_inference(model_artifact_path: InputPath('Model'),\n",
    "                    num_epochs: int,\n",
    "                    batch_size:int,\n",
    "                    learning_rate: float,\n",
    "                    train_dir: str,\n",
    "                    test_dir: str,\n",
    "                    model_name: str,\n",
    "                    test_loss: Output[HTML],\n",
    "                    metrics_json_path: OutputPath('Artifact')\n",
    "                    ) -> Dict[str, str]:\n",
    "\n",
    "    import torch\n",
    "    import model_builder, utils, model_inference, data_setup\n",
    "    import json\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from torchvision import transforms\n",
    "    from io import BytesIO\n",
    "    import base64\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "\n",
    "    # Setup hyperparameters\n",
    "    NUM_EPOCHS = num_epochs\n",
    "    BATCH_SIZE = batch_size\n",
    "    LEARNING_RATE = learning_rate\n",
    "\n",
    "    # Setup directories\n",
    "    TRAIN_DIR = train_dir\n",
    "    TEST_DIR = test_dir\n",
    "\n",
    "\n",
    "    # Create transforms\n",
    "    data_transform = transforms.Compose([\n",
    "      transforms.Resize((64, 64)),\n",
    "      transforms.ToTensor()\n",
    "    ])\n",
    "    # Create DataLoaders with help from data_setup.py\n",
    "    train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "        train_dir=TRAIN_DIR,\n",
    "        test_dir=TEST_DIR,\n",
    "        transform=data_transform,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )  \n",
    "    \n",
    "    # Set up device   \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load the model\n",
    "    model = model_builder.TinyVGG(input_shape=3,\n",
    "                                  hidden_units=10,\n",
    "                                  output_shape=3).to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_artifact_path))\n",
    "\n",
    "    # Set loss and optimizer\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                lr=LEARNING_RATE)\n",
    "\n",
    "    # Start training with help from engine.py\n",
    "    test_acc_last_epoch,test_loss_last_epoch,results = model_inference.test_result(model=model,\n",
    "                       test_dataloader=test_dataloader,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=NUM_EPOCHS,\n",
    "                       device=device)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    #Get number of epochs\n",
    "\n",
    "    epochs = range(len(df))\n",
    "        \n",
    "\n",
    "    # Plot train loss\n",
    "    tmpfile = BytesIO()\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.plot(epochs, df[\"test_loss\"], label={model_name})\n",
    "    plt.title(\"Test Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.savefig(tmpfile, format=\"png\")\n",
    "    tmpfile.seek(0)\n",
    "    encoded = base64.b64encode(tmpfile.read()).decode(\"utf-8\")\n",
    "    test_loss.path = f\"{test_loss.path}.html\"\n",
    "    html = f\"<img src='data:image/png;base64,{encoded}'>\"\n",
    "    with open(test_loss.path, 'w') as f:\n",
    "        f.write(html)\n",
    "    \n",
    "\n",
    "    # Below output parameters will be used by the mlflow to log the model in the next step.\n",
    "\n",
    "    with open(metrics_json_path, 'w') as f:\n",
    "        json.dump({'accuracy': test_acc_last_epoch, 'loss': test_loss_last_epoch}, f)\n",
    "        \n",
    "    return {\"model_name\" : model_name, \n",
    "            \"test_acc_last_epoch\": test_acc_last_epoch,\n",
    "            \"test_loss_last_epoch\": test_loss_last_epoch}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c3b6e62a-1f8d-4fb5-8a81-c261425f39ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building component using KFP package path: kfp==2.7.0\n",
      "Found 1 component(s) in file /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/register_model/register_model_component.py:\n",
      "Register model: ComponentInfo(name='Register model', function_name='register_model', func=<function register_model at 0x112536830>, target_image='mohitverma1688/register_model_component:v0.30', module_path=PosixPath('/Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/register_model/register_model_component.py'), component_spec=ComponentSpec(name='register-model', implementation=Implementation(container=ContainerSpecImplementation(image='mohitverma1688/register_model_component:v0.30', command=['sh', '-c', '\\nif ! [ -x \"$(command -v pip)\" ]; then\\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \\'mlflow\\' \\'GitPython\\' \\'numpy\\' && \"$0\" \"$@\"\\n', 'python3', '-m', 'kfp.dsl.executor_main'], args=['--executor_input', '{{$}}', '--function_to_execute', 'register_model'], env=None, resources=None), importer=None, graph=None), description=None, inputs={'parameters_json_path': InputSpec(type='system.Artifact@0.0.1', default=None, optional=False, is_artifact_list=False, description=None), 'metrics_json_path': InputSpec(type='system.Artifact@0.0.1', default=None, optional=False, is_artifact_list=False, description=None), 'model_artifact_path': InputSpec(type='system.Model@0.0.1', default=None, optional=False, is_artifact_list=False, description=None), 'experiment_name': InputSpec(type='String', default=None, optional=False, is_artifact_list=False, description=None), 'aws_access_key_id': InputSpec(type='String', default=None, optional=False, is_artifact_list=False, description=None), 'aws_secret_access_key': InputSpec(type='String', default=None, optional=False, is_artifact_list=False, description=None)}, outputs={'Output': OutputSpec(type='Dict', is_artifact_list=False, description=None)}, platform_spec=), output_component_file=None, base_image='mohitverma1688/model_train_component:v0.1', packages_to_install=['mlflow', 'GitPython', 'numpy'], pip_index_urls=None)\n",
      "Using base image: mohitverma1688/model_train_component:v0.1\n",
      "Using target image: mohitverma1688/register_model_component:v0.30\n",
      "Found existing file runtime-requirements.txt under /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/register_model.\n",
      "Overwriting existing file runtime-requirements.txt\n",
      "Generated file /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/register_model/runtime-requirements.txt.\n",
      "Found existing file .dockerignore under /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/register_model.\n",
      "Leaving this file untouched.\n",
      "Found existing file Dockerfile under /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/register_model.\n",
      "Leaving this file untouched.\n",
      "Building image mohitverma1688/register_model_component:v0.30 using Docker...\n",
      "Docker: Step 1/6 : FROM mohitverma1688/model_train_component:v0.1\n",
      "Docker:  ---> 81bbc346d378\n",
      "Docker: Step 2/6 : WORKDIR /usr/local/src/kfp/components\n",
      "Docker:  ---> Running in cf589018edbf\n",
      "Docker:  ---> 9cae0fb25355\n",
      "Docker: Step 3/6 : COPY runtime-requirements.txt runtime-requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!kfp component build src/components/register_model --component-filepattern register_model_component.py --no-push-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2e0cbada-bf0c-4aa4-802c-b5a83123b930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/components/register_model/register_model_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/components/register_model/register_model_component.py\n",
    " \n",
    "#Creating a python script to register the model in MLflow\n",
    "\n",
    "from typing import Dict, NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Dataset,Output,Artifact,OutputPath,InputPath, Model,Input\n",
    "\n",
    "@dsl.component(base_image='mohitverma1688/model_train_component:v0.1',\n",
    "               target_image='mohitverma1688/register_model_component:v0.30',\n",
    "               packages_to_install=['mlflow','GitPython','numpy']\n",
    "               )\n",
    "def register_model(parameters_json_path: InputPath('Artifact'),\n",
    "                   metrics_json_path: InputPath('Artifact'),\n",
    "                   model_artifact_path: InputPath('Model'),\n",
    "                   experiment_name: str,\n",
    "                   aws_access_key_id: str,\n",
    "                   aws_secret_access_key: str,\n",
    "                  ) -> dict:\n",
    "    \n",
    "    import mlflow\n",
    "    import torch\n",
    "    import json\n",
    "    import model_builder\n",
    "    import os\n",
    "    from mlflow.types import Schema, TensorSpec\n",
    "    import numpy as np\n",
    "    from mlflow.models import ModelSignature\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load the model\n",
    "    model = model_builder.TinyVGG(input_shape=3,\n",
    "                                  hidden_units=10,\n",
    "                                  output_shape=3).to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_artifact_path))\n",
    "    \n",
    "    # Log the model artifact in the MlFlow artifacts .\n",
    "\n",
    "    with open('/tmp/model_builder.py', 'w') as f:\n",
    "        f.write('''\n",
    "import torch\n",
    "from torch import nn\n",
    "class TinyVGG(nn.Module):\n",
    "  \"\"\"Creates the TinyVGG architecture.\n",
    "\n",
    "  Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n",
    "  See the original architecture here: https://poloclub.github.io/cnn-explainer/\n",
    "\n",
    "  Args:\n",
    "    input_shape: An integer indicating number of input channels.\n",
    "    hidden_units: An integer indicating number of hidden units between layers.\n",
    "    output_shape: An integer indicating number of output units.\n",
    "  \"\"\"\n",
    "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "      super().__init__()\n",
    "      self.conv_block_1 = nn.Sequential(\n",
    "          nn.Conv2d(in_channels=input_shape,\n",
    "                    out_channels=hidden_units,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=hidden_units,\n",
    "                    out_channels=hidden_units,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2,\n",
    "                        stride=2)\n",
    "      )\n",
    "      self.conv_block_2 = nn.Sequential(\n",
    "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(2)\n",
    "      )\n",
    "      self.classifier = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          # Where did this in_features shape come from?\n",
    "          # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
    "          nn.Linear(in_features=hidden_units*13*13,\n",
    "                    out_features=output_shape)\n",
    "      )\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "      x = self.conv_block_1(x)\n",
    "      x = self.conv_block_2(x)\n",
    "      x = self.classifier(x)\n",
    "      return x\n",
    "      # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion\n",
    "            ''')\n",
    "\n",
    "    # Load the parameters and metrics for mlflow to register. \n",
    "    with open(metrics_json_path) as f:\n",
    "        metrics = json.load(f)\n",
    "        print(metrics)\n",
    "    with open(parameters_json_path) as f:\n",
    "        parameters = json.load(f)\n",
    "        print(parameters)\n",
    "\n",
    "    #Define the mlflow tracking URI and experiment name\n",
    "    tracking_uri = 'http://my-mlflow.kubeflow:5000'\n",
    "    experiment_name = experiment_name\n",
    "    mlflow.tracking.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    reg_model_name = \"CNN-TinyVG-Model\"\n",
    "\n",
    "    # Logs the enviornment variable to be used by minio\n",
    "    os.environ[\"GIT_PYTHON_REFRESH\"] = \"quiet\"\n",
    "    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio-service.kubeflow:9000\"\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = aws_access_key_id\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = aws_secret_access_key\n",
    "\n",
    "    #Define the model schema\n",
    "\n",
    "    input_schema = Schema([TensorSpec(np.dtype(np.float32), (1, 3, 64,64))])\n",
    "    output_schema = Schema([TensorSpec(np.dtype(np.float32), (1, 3))])\n",
    "    signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "    \n",
    "\n",
    "    # Star the mlflow server to register the model \n",
    "    with mlflow.start_run() as run:\n",
    "        for metric in metrics:\n",
    "            mlflow.log_metric(metric, metrics[metric])\n",
    "        mlflow.log_params(parameters)\n",
    "\n",
    "        artifact_path = \"CNN-TinyVG\"\n",
    "        mlflow.pytorch.log_model(model,\n",
    "                                 registered_model_name=reg_model_name,\n",
    "                                 signature=signature,\n",
    "                                 artifact_path=artifact_path,\n",
    "                                code_paths=['/tmp/model_builder.py'])\n",
    "\n",
    "        # Example snippet to log the scripted model \n",
    "        #scripted_pytorch_model = torch.jit.script(model)\n",
    "        \n",
    "        #mlflow.pytorch.log_model(scripted_pytorch_model, \"scripted_model\")\n",
    "        \n",
    "        run_id = mlflow.last_active_run().info.run_id\n",
    "        print(f\"Logged data and model in run {run_id}...\")\n",
    "\n",
    "        # Run time model uri \n",
    "        model_uri = f\"runs:/{run_id}/CNN-model\"\n",
    "        \n",
    "        #Another way of loading model at runtime\n",
    "        #loaded_model = mlflow.pytorch.load_model(model_uri)\n",
    "        \n",
    "        return {\"artifact_path\": artifact_path, \"artifact_uri\": run.info.artifact_uri}\n",
    "        \n",
    "\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9cf3cf89-473f-49ea-becb-26cbaf030eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building component using KFP package path: kfp==2.7.0\n",
      "Found 1 component(s) in file /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_eval/model_eval_component.py:\n",
      "Predict on sample image: ComponentInfo(name='Predict on sample image', function_name='predict_on_sample_image', func=<function predict_on_sample_image at 0x111386830>, target_image='mohitverma1688/model_eval_component:v0.8', module_path=PosixPath('/Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_eval/model_eval_component.py'), component_spec=ComponentSpec(name='predict-on-sample-image', implementation=Implementation(container=ContainerSpecImplementation(image='mohitverma1688/model_eval_component:v0.8', command=['sh', '-c', '\\nif ! [ -x \"$(command -v pip)\" ]; then\\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \\'mlflow\\' \\'GitPython\\' && \"$0\" \"$@\"\\n', 'python3', '-m', 'kfp.dsl.executor_main'], args=['--executor_input', '{{$}}', '--function_to_execute', 'predict_on_sample_image'], env=None, resources=None), importer=None, graph=None), description=None, inputs={'test_dir': InputSpec(type='String', default=None, optional=False, is_artifact_list=False, description=None), 'model_info': InputSpec(type='Dict', default=None, optional=False, is_artifact_list=False, description=None), 'image_path': InputSpec(type='String', default=None, optional=False, is_artifact_list=False, description=None), 'aws_access_key_id': InputSpec(type='String', default=None, optional=False, is_artifact_list=False, description=None), 'aws_secret_access_key': InputSpec(type='String', default=None, optional=False, is_artifact_list=False, description=None)}, outputs={'model_uri': OutputSpec(type='String', is_artifact_list=False, description=None), 'pred_label_class': OutputSpec(type='String', is_artifact_list=False, description=None)}, platform_spec=), output_component_file=None, base_image='mohitverma1688/model_train_component:v0.1', packages_to_install=['mlflow', 'GitPython'], pip_index_urls=None)\n",
      "Using base image: mohitverma1688/model_train_component:v0.1\n",
      "Using target image: mohitverma1688/model_eval_component:v0.8\n",
      "Found existing file runtime-requirements.txt under /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_eval.\n",
      "Overwriting existing file runtime-requirements.txt\n",
      "Generated file /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_eval/runtime-requirements.txt.\n",
      "Found existing file .dockerignore under /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_eval.\n",
      "Leaving this file untouched.\n",
      "Found existing file Dockerfile under /Users/mverma/Documents/STUDY-Mac/data-science/sample_project/python-containerized1/src/components/model_eval.\n",
      "Leaving this file untouched.\n",
      "Building image mohitverma1688/model_eval_component:v0.8 using Docker...\n",
      "Docker: Step 1/6 : FROM mohitverma1688/model_train_component:v0.1\n",
      "Docker:  ---> 81bbc346d378\n",
      "Docker: Step 2/6 : WORKDIR /usr/local/src/kfp/components\n",
      "Docker:  ---> Running in dbbbf9bee459\n",
      "Docker:  ---> 0e299d47833d\n",
      "Docker: Step 3/6 : COPY runtime-requirements.txt runtime-requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!kfp component build src/components/model_eval --component-filepattern model_eval_component.py --no-push-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "204adea5-2234-4836-9a5c-f643b014ea98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/components/model_eval/model_eval_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/components/model_eval/model_eval_component.py\n",
    "from kfp import dsl\n",
    "from typing import NamedTuple\n",
    "from kfp.dsl import Dataset,Output,Artifact,OutputPath,InputPath, Model,Input\n",
    "\n",
    "@dsl.component(base_image='mohitverma1688/model_train_component:v0.1',\n",
    "               target_image='mohitverma1688/model_eval_component:v0.8',\n",
    "               packages_to_install=['mlflow','GitPython']\n",
    "               )\n",
    "def predict_on_sample_image(test_dir: str, \n",
    "                         model_info: dict, \n",
    "                         image_path: str,\n",
    "                         aws_access_key_id: str, \n",
    "                         aws_secret_access_key: str ,\n",
    "                         ) -> NamedTuple('outputs', [('model_uri', str),('pred_label_class', str)]):\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import utils\n",
    "    import os\n",
    "    import mlflow\n",
    "    import json\n",
    "\n",
    "    # Set Minio credentials in the environment\n",
    "    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio-service.kubeflow:9000\"\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = aws_access_key_id\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = aws_secret_access_key\n",
    "\n",
    "    \n",
    "    # Set up the class names, below code uses utis.py to get the class_names from the dataset\n",
    "    \n",
    "    DATA_DIR = test_dir \n",
    "    class_names = utils.class_names_found(DATA_DIR)\n",
    "    #print(class_names)\n",
    "     \n",
    "    #class_names = [\"pizza\", \"steak\", \"sushi\"]\n",
    "    \n",
    "    # Set up device   \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    \n",
    "    artifact_path = model_info[\"artifact_path\"]\n",
    "    artifact_uri = model_info[\"artifact_uri\"]\n",
    "\n",
    "    # Loading the model from the mlFlow artifact repository \n",
    "    \n",
    "    mlflow.set_tracking_uri(\"http://my-mlflow.kubeflow:5000\")\n",
    "    model_uri = f\"{artifact_uri}/{artifact_path}\"\n",
    "    model = mlflow.pytorch.load_model(model_uri)\n",
    "\n",
    "    # Below section is to use the the random path if the image path is not provided. A random image from test data in dataset is selected for prediction. \n",
    "    # Make sure that the downloaded data has the path for testing some images for example \"/data/predict/123.jpg\"\n",
    "    \n",
    "    if image_path == None:\n",
    "    \n",
    "        test_data_paths =  list(Path(\"/data/test\").glob(\"*/*.jpg\"))\n",
    "        image_path = random.choice(test_data_paths)\n",
    "        print(image_path)\n",
    "\n",
    "    # Load in the image and turn it into torch.float32 ( same type as model)\n",
    "    image = torchvision.io.read_image(image_path).type(torch.float32)\n",
    "\n",
    "    # Preprocess the image to get between 0 and 1\n",
    "    image = image / 255.\n",
    "\n",
    "    # Resize the image to be of same size as model.\n",
    "    transform = torchvision.transforms.Resize(size=(64,64))\n",
    "    image = transform(image)\n",
    "\n",
    "    # Predict on image\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Put image on the model\n",
    "        image = image.to(device)\n",
    "\n",
    "        # Get the pred_logits\n",
    "        pred_logits = model(image.unsqueeze(dim=0))  # Adding a new dimension for the batch size.\n",
    "\n",
    "        # Get the pred probs\n",
    "        pred_prob = torch.softmax(pred_logits, dim=1)\n",
    "\n",
    "        # Get the pred_labels\n",
    "        pred_label = torch.argmax(pred_prob, dim=1)\n",
    "        pred_label_class = class_names[pred_label]\n",
    "\n",
    "    print(f\"[INFO] Pred class: {pred_label_class}, Pred_prob: {pred_prob.max():.3f}\")\n",
    "    pred_prob_max = pred_prob.max()\n",
    "    print(type(pred_prob_max))\n",
    "\n",
    "    outputs = NamedTuple(\"outputs\", model_uri=str, pred_label_class=str)\n",
    "    \n",
    "    return outputs(model_uri, pred_label_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696181be-7181-48b2-a112-e55001d430e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfff21fa-9a36-4dcd-8708-be5733d11fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/components/model_deployment/model_deployment_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/components/model_deployment/model_deployment_component.py\n",
    "from kfp import dsl\n",
    "@dsl.component(\n",
    "    packages_to_install=[\"kserve==0.12.0\",\"ray[serve]<=2.9.3,>=2.9.2\"],\n",
    "    base_image=\"python:3.10\",\n",
    ")\n",
    "def model_serving(model_uri: str):\n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1TorchServeSpec\n",
    "    from kserve import V1beta1ModelSpec\n",
    "    from kserve import V1beta1ModelFormat\n",
    "    import os\n",
    "\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "    \n",
    "    name='cnn-tinyvg-v1'\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "    isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                                   kind=constants.KSERVE_KIND,\n",
    "                                   metadata=client.V1ObjectMeta(\n",
    "                                       name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "                                   spec=V1beta1InferenceServiceSpec(\n",
    "                                   predictor=V1beta1PredictorSpec( model=V1beta1ModelSpec(model_format=V1beta1ModelFormat(name='mlflow')),\n",
    "                                   service_account_name='mlflow-sa',\n",
    "                                   pytorch=(V1beta1TorchServeSpec(storage_uri=model_uri)))))\n",
    "    KServe = KServeClient()\n",
    "    KServe.create(isvc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e7dc965d-84d3-48f2-8e2e-5eebe31b5d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline.py\n",
    "from kfp import kubernetes\n",
    "from kubernetes import client, config\n",
    "import base64\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "from src.components.data_download.data_download_component import dataset_download\n",
    "from src.components.model_train_cnn.model_train_component import model_train\n",
    "from src.components.model_inference.model_inference_component import model_inference\n",
    "from src.components.register_model.register_model_component import register_model\n",
    "from src.components.model_eval.model_eval_component import predict_on_sample_image\n",
    "#from src.components.model_deployment.model_deployment_component import model_serving\n",
    "\n",
    "\n",
    "BASE_PATH=\"/data\"\n",
    "URL=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\"\n",
    "#INPUT_BUCKET=\"datanewbucket\"\n",
    "NUM_EPOCHS=10\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_UNITS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "MODEL_NAME = \"cnn_tinyvg_v1\"\n",
    "MODEL_DIR = \"/data/models\"\n",
    "EXPORT_BUCKET = \"modeloutput\"\n",
    "TRAIN_DIR = \"/data/train\"\n",
    "TEST_DIR = \"/data/test\"\n",
    "IMAGE_PATH = \"/data/test/pizza/1152100.jpg\"\n",
    "EXPERIMENT_NAME = \"CNN-TinyVG-Demo-exp1\"\n",
    "\n",
    "@dsl.pipeline(name='CNN-TinyVG-Demo',\n",
    "              description='This pipeline is a demo for training,evaluating and deploying Convutional Neural network',\n",
    "              display_name='Kubeflow-MlFLow-Demo')\n",
    "\n",
    "\n",
    "\n",
    "def kubeflow_pipeline(base_path: str = BASE_PATH,\n",
    "                     url:str = URL,\n",
    "                     batch_size:int = BATCH_SIZE,\n",
    "                     train_dir:str = TRAIN_DIR,\n",
    "                     test_dir:str = TEST_DIR,\n",
    "                     num_epochs: int = NUM_EPOCHS,\n",
    "                     hidden_units:int = HIDDEN_UNITS,\n",
    "                     learning_rate:float = LEARNING_RATE,\n",
    "                     model_name: str = MODEL_NAME,\n",
    "                     model_dir: str = MODEL_DIR,\n",
    "                     export_bucket: str = EXPORT_BUCKET,\n",
    "                     image_path: str = IMAGE_PATH,\n",
    "                     experiment_name: str = EXPERIMENT_NAME\n",
    "                     ):\n",
    "    # Load Kubernetes configuration\n",
    "    config.load_kube_config()\n",
    "\n",
    "    # Fetch the Minio credentials from the secret\n",
    "\n",
    "    secret_name = \"minio-credentials\"\n",
    "    secret_namespace = \"kubeflow\"\n",
    "    secret_key_id = \"AWS_ACCESS_KEY_ID\"\n",
    "    secret_key_access = \"AWS_SECRET_ACCESS_KEY\"\n",
    "\n",
    "    v1 = client.CoreV1Api()\n",
    "    secret = v1.read_namespaced_secret(secret_name, namespace=secret_namespace)\n",
    "\n",
    "\n",
    "    # Convert bytes to string\n",
    "    aws_access_key_id = base64.b64decode(secret.data[secret_key_id]).decode('utf-8')\n",
    "    aws_secret_access_key = base64.b64decode(secret.data[secret_key_access]).decode('utf-8')\n",
    "   \n",
    "    pvc1 = kubernetes.CreatePVC(\n",
    "        # can also use pvc_name instead of pvc_name_suffix to use a pre-existing PVC\n",
    "        pvc_name='kubeflow-pvc8',\n",
    "        access_modes=['ReadWriteOnce'],\n",
    "        size='500Mi',\n",
    "        storage_class_name='standard',\n",
    "    )\n",
    "    \n",
    "    task1 = dataset_download(base_path=base_path,\n",
    "                            url=url,\n",
    "                            )\n",
    "    task1.set_caching_options(False)\n",
    "    \n",
    "    task2 = model_train(batch_size=batch_size,\n",
    "                        num_epochs=num_epochs,\n",
    "                        train_dir=train_dir,\n",
    "                        test_dir=test_dir,\n",
    "                        hidden_units=hidden_units,\n",
    "                        learning_rate=learning_rate,\n",
    "                        model_name=model_name,\n",
    "                        model_dir=model_dir,\n",
    "                        export_bucket=export_bucket,\n",
    "                        ).after(task1)\n",
    "    task2.set_caching_options(False)\n",
    "    \n",
    "    task3 = model_inference(test_dir=test_dir,\n",
    "                           model_artifact_path=task2.outputs[\"model_artifact_path\"],\n",
    "                           train_dir=train_dir,\n",
    "                           learning_rate=learning_rate,\n",
    "                           batch_size=batch_size,\n",
    "                           num_epochs=num_epochs,\n",
    "                           model_name=model_name\n",
    "                           ).after(task2)\n",
    "    task3.set_caching_options(False)\n",
    "    \n",
    "    task4 = register_model(model_artifact_path=task2.outputs[\"model_artifact_path\"],\n",
    "                           parameters_json_path=task2.outputs[\"parameters_json_path\"],\n",
    "                           metrics_json_path=task3.outputs[\"metrics_json_path\"],\n",
    "                           aws_access_key_id=aws_access_key_id,\n",
    "                           aws_secret_access_key=aws_secret_access_key,                         \n",
    "                           experiment_name=experiment_name).after(task3)\n",
    "    task4.set_caching_options(False)\n",
    "\n",
    "    task5 = predict_on_sample_image(test_dir=test_dir,\n",
    "                                 image_path=image_path,\n",
    "                                 model_info=task4.output,\n",
    "                                 aws_access_key_id=aws_access_key_id,\n",
    "                                 aws_secret_access_key=aws_secret_access_key).after(task4)\n",
    "    task5.set_caching_options(False)\n",
    "\n",
    "    \n",
    "\n",
    "#    task6 = model_serving(model_uri=task5.outputs['model_uri']).after(task5)\n",
    "#    task6.set_caching_options(False)\n",
    "    \n",
    "              \n",
    "                                                  \n",
    "    kubernetes.mount_pvc(\n",
    "        task1,\n",
    "        pvc_name=pvc1.outputs['name'],\n",
    "        mount_path='/data',\n",
    "    )\n",
    "    kubernetes.mount_pvc(\n",
    "        task2,\n",
    "        pvc_name=pvc1.outputs['name'],\n",
    "        mount_path='/data',\n",
    "    )\n",
    "    kubernetes.mount_pvc(\n",
    "        task3,\n",
    "        pvc_name=pvc1.outputs['name'],\n",
    "        mount_path='/data',\n",
    "    )\n",
    "    kubernetes.mount_pvc(\n",
    "        task4,\n",
    "        pvc_name=pvc1.outputs['name'],\n",
    "        mount_path='/data',\n",
    "    )\n",
    "    kubernetes.mount_pvc(\n",
    "        task5,\n",
    "        pvc_name=pvc1.outputs['name'],\n",
    "        mount_path='/data',\n",
    "    )\n",
    "#    kubernetes.mount_pvc(\n",
    "#        task6,\n",
    "#        pvc_name=pvc1.outputs['name'],\n",
    "#        mount_path='/data',\n",
    "#    )\n",
    "    delete_pvc1 = kubernetes.DeletePVC(pvc_name=pvc1.outputs['name']).after(task5)\n",
    "\n",
    "compiler.Compiler().compile(kubeflow_pipeline, 'kubeflow-demo2.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae2e5b6d-b56d-4cb5-b20b-bef326cbfe52",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'kfp.dsl' has no attribute 'ResourceOp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dsl\n\u001b[0;32m----> 2\u001b[0m deploy_step \u001b[38;5;241m=\u001b[39m \u001b[43mdsl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResourceOp\u001b[49m(\n\u001b[1;32m      3\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseldondeploy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         k8s_resource\u001b[38;5;241m=\u001b[39mseldon_config,\n\u001b[1;32m      5\u001b[0m         attribute_outputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m.metadata.name}\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'kfp.dsl' has no attribute 'ResourceOp'"
     ]
    }
   ],
   "source": [
    "from kfp import dsl\n",
    "deploy_step = dsl.ResourceOp(\n",
    "        name=\"seldondeploy\",\n",
    "        k8s_resource=seldon_config,\n",
    "        attribute_outputs={\"name\": \"{.metadata.name}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c47195ce-a674-4979-b904-eaf37229fe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: ray[serve]\n"
     ]
    }
   ],
   "source": [
    "!pip install ray[serve]<=2.9.3,>=2.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0365e045-abcd-41ae-8ae9-1fd65ade8b52",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RayServeHandle' from 'ray.serve.handle' (/Users/mverma/Documents/STUDY-Mac/data-science/sample_project/env/lib/python3.10/site-packages/ray/serve/handle.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkserve\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/STUDY-Mac/data-science/sample_project/env/lib/python3.10/site-packages/kserve/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_server\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelServer\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InferenceServerClient\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfer_type\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InferRequest, InferInput, InferResponse, InferOutput\n",
      "File \u001b[0;32m~/Documents/STUDY-Mac/data-science/sample_project/env/lib/python3.10/site-packages/kserve/model_server.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serve \u001b[38;5;28;01mas\u001b[39;00m rayserve\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserve\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Deployment\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserve\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhandle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RayServeHandle\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KSERVE_LOG_CONFIG, logger\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'RayServeHandle' from 'ray.serve.handle' (/Users/mverma/Documents/STUDY-Mac/data-science/sample_project/env/lib/python3.10/site-packages/ray/serve/handle.py)"
     ]
    }
   ],
   "source": [
    "import kserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad4757-d546-4c33-9d72-72ed5a9c1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.py\n",
    "from kfp.client import Client\n",
    "\n",
    "client = Client(host='http://localhost:8002')\n",
    "run = client.create_run_from_pipeline_package(\n",
    "    'kubeflow-demo2.yaml',\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf4685b-6f52-4fea-925a-1a2b78f16a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {'train_loss': [1.1130876988172531, 1.1088187843561172, 1.0958865135908127, 1.0962762534618378, 1.0938437581062317, 1.1108780950307846, 1.0907627046108246, 1.1078475266695023, 1.1044471561908722, 1.09113809466362], 'train_acc': [0.2890625, 0.29296875, 0.4140625, 0.29296875, 0.41796875, 0.3046875, 0.42578125, 0.3046875, 0.3046875, 0.42578125], 'test_loss': [1.094068129857381, 1.107581655184428, 1.1117815176645915, 1.1201920906702678, 1.1264954408009846, 1.128881613413493, 1.1216977437337239, 1.1192431847254436, 1.1100176175435383, 1.1010971864064534], 'test_acc': [0.19791666666666666, 0.19791666666666666, 0.19791666666666666, 0.19791666666666666, 0.2604166666666667, 0.2604166666666667, 0.2604166666666667, 0.2604166666666667, 0.2604166666666667, 0.2604166666666667]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f57561-2001-4bf1-9f81-0bd7dff9a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in x.items():\n",
    "    results = { }\n",
    "    value = value[9]\n",
    "    result[key] = value\n",
    "result\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e82c2c-b291-4a62-9c24-9493f7791e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import Dataset\n",
    "from kfp.dsl import Input\n",
    "\n",
    "@dsl.component\n",
    "def print_artifact_properties(dataset: Input[Dataset]):\n",
    "    with open(dataset.path) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    print('Information about the artifact')\n",
    "    print('Name:', dataset.name)\n",
    "    print('URI:', dataset.uri)\n",
    "    print('Path:', dataset.path)\n",
    "    print('Metadata:', dataset.metadata)\n",
    "    \n",
    "    return len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca38300-4c5d-448e-8182-36f35095d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 5\n",
    "NUM_EPOCH= NUM_EPOCH - 1\n",
    "NUM_EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aaaccd-e557-4a37-93a9-8abba57ddff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f10c3-f8a7-4d3f-babc-0a9eeabbed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [2, 3, 4, 0, 42, 17]\n",
    "numbers = numbers.sort()\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe181f03-a6d3-4fe0-93be-c293927c79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "-> Dict[str, list] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c22ba-001d-451a-a925-e8c9ef4f3b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall kserve -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44793b79-c928-4809-9ec8-80ee7c3f755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import Dataset\n",
    "from kfp.dsl import Input\n",
    "\n",
    "@dsl.component\n",
    "def print_artifact_properties(dataset: Input[Dataset]):\n",
    "    with open(dataset.path) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    print('Information about the artifact')\n",
    "    print('Name:', dataset.name)\n",
    "    print('URI:', dataset.uri)\n",
    "    print('Path:', dataset.path)\n",
    "    print('Metadata:', dataset.metadata)\n",
    "    \n",
    "    return len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcbf996-fab2-4b4a-9369-7384a6eaaac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb15e046-fcf0-496b-b64e-075584dbf4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: ray[serve]\n"
     ]
    }
   ],
   "source": [
    "!pip install ray[serve]<=2.9.3,>=2.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0f6ea6db-edf6-4de3-96c6-0c596b4b8a47",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/test/pizza/1152100.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [op]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 36\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/test/pizza/1152100.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     im_file \u001b[38;5;241m=\u001b[39m BytesIO()\n\u001b[1;32m     38\u001b[0m     img\u001b[38;5;241m.\u001b[39msave(im_file, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJPEG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/STUDY-Mac/data-science/sample_project/env/lib/python3.10/site-packages/PIL/Image.py:3247\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3244\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3248\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/test/pizza/1152100.jpg'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from io import BytesIO\n",
    "import PIL.Image as Image\n",
    "import base64\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import mlflow\n",
    "\n",
    "class Inference(object):\n",
    "    def __init__(self):\n",
    "        self.device = 'cpu'\n",
    "        mlflow.set_tracking_uri(\"http://20.28.195.42/mlflow/\")\n",
    "        mlflow.set_registry_uri(\"http://20.28.195.42/mlflow/\")\n",
    "        self.model_path = f\"runs:/{os.environ['RUN_ID']}/models\"\n",
    "        self.model = mlflow.pytorch.load_model(self.model_path)\n",
    "        print(self.model)\n",
    "\n",
    "    def predict(self, X, feature_names):\n",
    "        X = X[0].encode()\n",
    "        im_bytes = base64.b64decode(X)\n",
    "        im_file = BytesIO(im_bytes)\n",
    "        img = Image.open(im_file)\n",
    "        img = np.asarray(img.resize((128, 128))).astype(np.float32) / 255.0\n",
    "        if img.ndim == 2:\n",
    "            img = np.stack([img, img, img], axis=-1)\n",
    "        img = np.transpose(img, (-1, 0, 1))\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = torch.from_numpy(img).to(self.device)\n",
    "        op = torch.nn.Sigmoid()(self.model(img))\n",
    "        op = torch.where(op > 0.5, 1, 0).item()\n",
    "        return [op]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    img = Image.open(\"/data/test/pizza/1152100.jpg\")\n",
    "    im_file = BytesIO()\n",
    "    img.save(im_file, format=\"JPEG\")\n",
    "    im_bytes = im_file.getvalue()  # im_bytes: image in binary format.\n",
    "    im_b64 = base64.b64encode(im_bytes)\n",
    "    im_b64 = im_b64.decode()\n",
    "    op = Inference().predict(im_b64, [\"Image\"])\n",
    "    print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c8a7889b-2c3d-43a0-bf56-6709ceb7367c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfp                                2.7.0\n",
      "kfp-kubernetes                     1.2.0\n",
      "kfp-pipeline-spec                  0.3.0\n",
      "kfp-server-api                     2.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep kfp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
